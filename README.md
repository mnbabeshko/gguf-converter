# GGUF Converter

<p align="center">
  <img src="images/logoLLM.png" alt="GGUF Converter Logo" width="160">
</p>

<p align="center">
  <img src="images/screenshot.png" alt="GGUF Converter Interface" width="600">
</p>

Universal utility for converting AI models to GGUF format with quantization.

[ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ](README_RU.md)

## Features

- ğŸ”„ Convert models from `.safetensors`, `.pt`, `.pth`, `.bin` formats
- ğŸ“Š 8 quantization levels (Q3_K_S to Q8_0)
- ğŸ“ Automatic Downloads folder scanning
- ğŸ” Smart model name extraction from metadata
- ğŸ“‚ **Custom output folder** (persisted between sessions)
- ğŸ“¦ **Batch conversion** of multiple files
- ğŸ” **GGUF file inspection** (view tensors, export to CSV)
- ğŸŒ Multilingual: English, Ğ ÑƒÑÑĞºĞ¸Ğ¹, ä¸­æ–‡
- âš¡ FP8 model processing (automatic dequantization)
- ğŸ¯ Mixed quantization Q4_K_M (important layers in Q6_K)

## Installation

### Requirements

- Python 3.8+
- Windows 10/11

### Dependencies

```bash
pip install numpy pillow safetensors pywin32
```

Optional for PyTorch models:
```bash
pip install torch
```

## Usage

### Launch

```bash
python gguf_converter.py
```

Or use `run_converter.bat` for quick launch.

### Conversion Process

1. Select a model from the list (Downloads folder is scanned automatically)
2. Or click "Browse" to select a file manually
3. Choose output folder (defaults to Downloads)
4. Select quantization level
5. Click "Convert"
6. The converted file will appear in the selected folder

### Batch Conversion

1. Enable "Batch mode" checkbox
2. Select multiple files in the dialog
3. Click "Convert"
4. All files will be processed sequentially
5. A summary will appear at the end

### GGUF File Inspection

1. Click "Inspect GGUF" button
2. Select a GGUF file
3. View tensor information:
   - Name, shape, data type, size
   - Total tensor count
   - GGUF format version
4. Export to CSV or copy to clipboard

## Quantization Levels

| Type | Description | Size |
|------|-------------|------|
| Q3_K_S | 3-bit small | ~2.5 bits/weight |
| Q3_K_M | 3-bit medium | ~3 bits/weight |
| Q4_K_S | 4-bit small | ~4 bits/weight |
| Q4_K_M | 4-bit medium | ~4.5 bits/weight â­ |
| Q5_K_S | 5-bit small | ~5 bits/weight |
| Q5_K_M | 5-bit medium | ~5.5 bits/weight |
| Q6_K | 6-bit | ~6 bits/weight |
| Q8_0 | 8-bit | ~8 bits/weight |

â­ Q4_K_M â€” recommended balance of quality and size (uses mixed quantization: important layers in Q6_K)

## Project Structure

```
gguf-converter-v1/
â”œâ”€â”€ gguf_converter.py    # Main script
â”œâ”€â”€ quantizer.py         # Quantization logic
â”œâ”€â”€ ui_widgets.py        # UI components
â”œâ”€â”€ translations.py      # Translations (RU/EN/ZH)
â”œâ”€â”€ settings.json        # User settings
â”œâ”€â”€ run_converter.bat    # Windows launcher
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ logoLLM.png      # Logo
â”‚   â”œâ”€â”€ logoLLM.ico      # Window icon
â”‚   â””â”€â”€ nayan.gif        # Nyan Cat animation
â”œâ”€â”€ music/               # Music folder
â”‚   â””â”€â”€ *.mp3            # MP3 files for background
â”œâ”€â”€ tools/               # Analysis utilities
â”‚   â”œâ”€â”€ analyze_source_model.py    # Safetensors analysis
â”‚   â”œâ”€â”€ check_quantization.py      # Quantization check
â”‚   â”œâ”€â”€ compare_gguf_models.py     # GGUF comparison
â”‚   â”œâ”€â”€ compare_tensors_detailed.py # Detailed comparison
â”‚   â”œâ”€â”€ compare_two_models.py      # Two model comparison
â”‚   â””â”€â”€ test_quantization_types.py # Quantization tests
â””â”€â”€ README.md            # Documentation
```

## Music

Place MP3 or WAV files in the `music/` folder for background music.
The ğŸ”Š button toggles sound on/off. Music plays only during conversion.

## Features

### Double Launch Protection
The utility uses Windows Mutex to prevent running multiple instances.

### Smart Model Name Detection
Output filename is extracted in priority order:
1. **Safetensors metadata** (highest priority):
   - `modelspec.title` â€” standard specification
   - `ss_output_name` â€” Kohya trainer
   - `ss_sd_model_name` â€” Kohya SD model
   - `model_name`, `name`, `title` â€” common keys
2. **config.json** â€” for generic files (model.safetensors)
3. **Cleaned filename** â€” suffixes like `-fp16`, `_pruned` are removed
4. **Folder name** â€” only if file has generic name

### FP8 Model Processing
- Automatic FP8 weight detection
- FP8 â†’ FP32 dequantization before quantization
- Service tensor filtering (scale, zero_point)

### Mixed Quantization (Q4_K_M)
Q4_K_M uses intelligent quantization:
- Important layers (attention, first blocks) â†’ Q6_K
- Other layers â†’ Q4_K
- Bias and normalization â†’ F32

### Dark Theme
The interface uses a dark color scheme, comfortable for the eyes.
Dark window title bar is supported on Windows 11.

## Changelog

### v1.8
- âš¡ Progress bar animation optimization (3x faster)
- ğŸ”§ Pre-created canvas elements instead of delete/create cycle
- ğŸµ Fixed music playback - now random track after completion
- ğŸ› Fixed percentage cutoff on the right
- ğŸ¬ Independent animation thread - smooth animation regardless of CPU load

### v1.7
- âš¡ Multi-threaded quantization (uses all CPU cores)
- âš¡ Vectorized quantization functions (5-12x speedup)
- ğŸ”§ Async UI updates via queue
- ğŸ› Removed torch.cuda.empty_cache() calls (were slowing down)

### v1.2
- âœ¨ Output folder selection (saved in settings)
- âœ¨ Batch conversion of multiple files
- âœ¨ GGUF file inspection (view tensors, export CSV)
- ğŸ› Fixed crash on Q3 quantization
- ğŸ› Fixed bit packing in Q5_K and Q6_K
- ğŸ“ Added tensor count to log output

### v1.1
- âœ¨ Multilingual support (RU/EN/ZH)
- âœ¨ FP8 model processing
- âœ¨ Mixed quantization Q4_K_M
- ğŸ¨ Improved interface

### v1.0
- ğŸ‰ Initial release

## License

MIT License

## Author

miha2017

---

## ğŸ’° Donate

If you find this project useful, you can support development:

**Cryptocurrency:**

<p align="center">
  <img src="images/QR-tron-donate.png" alt="USDT TRC20 QR Code" width="150">
</p>

**USDT (TRC20):** `TFZoJGcYd8z2QPokiZSBcZnrkTevEnxpyR`

---

## âš¡ Contact:

https://t.me/mnbabeshko

*GGUF Converter v1.8*
